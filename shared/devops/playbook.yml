- name: Provision the Vagrant machine
  hosts: localhost
  connection: local
  become: yes
  gather_facts: yes

  vars:

    spark_release: spark-2.1.0-bin-hadoop2.6
    spark_archive_url: http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.6.tgz
    spark_md5: 'md5:18 67 65 A9 58 38 60 2E  D8 1E A4 E9 27 05 61 F8'

    hadoop_release: hadoop-2.6.0
    hadoop_archive: '{{ hadoop_release }}.tar.gz'
    hadoop_sha256: '7a2ef6e7f468afcae95d0f7214816033c7e5c7982454061ccb117896d58d279f'

    zookeeper_archive_url: http://apache.org/dist/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz
    zookeeper_release: zookeeper-3.4.9
    zookeeper_md5: 3e8506075212c2d41030d874fcc9dcd2
    zookeeper_data_dir: /var/app/zookeeper

    accumulo_archive_url: http://mirror.jax.hugeserver.com/apache/accumulo/1.7.2/accumulo-1.7.2-bin.tar.gz
    accumulo_release: accumulo-1.7.2
    accumulo_md5: de876f3f6df4a9659635378ae7df1b86
    accumulo_secret: nimbus

    hbase_release: 1.2.4
    hbase_archive: 'hbase-{{ hbase_release }}-bin.tar.gz'
    hbase_md5: '26 CF 30 B9 FC 01 16 7B  A9 8F C6 37 E8 60 6D 5C'
    hbase_home_dir: '/srv/hbase-{{ hbase_release }}'
    hbase_data_dir: /var/app/hbase


  tasks:
#
#  - name: Add a directory for Ansible callback plugins
#    file: path=/usr/share/ansible/plugins/callback state=directory
#
#  - name: Add module to create human readable output
#    template:
#      src: /home/vagrant/shared/devops/human_log.py
#      dest: /usr/share/ansible/plugins/callback
#
#  - name: Add pretty-print module to Ansible
#    blockinfile:
#      dest: /etc/ansible/ansible.cfg
#      marker: "# {mark} ANSIBLE MANAGED CALLBACK PLUGIN BLOCK "
#      insertafter: '#callback_plugins   = /usr/share/ansible/plugins/callback'
#      content: |
#        callback_plugins   = /usr/share/ansible/plugins/callback
#
#  - name: Prevent 'retry' file creation
#    blockinfile:
#      dest: /etc/ansible/ansible.cfg
#      marker: "# {mark} ANSIBLE MANAGED RETRY BLOCK"
#      insertbefore: "#retry_files_enabled = False"
#      content: |
#        retry_files_enabled = False
#
#  - name: ensure apt cache is up to date
#    apt: update_cache=yes cache_valid_time=86400
#
#  - name: dist-upgrade
#    apt: upgrade=dist cache_valid_time=86400
#
#  - name: Ensure required packages are installed
#    apt: name={{item}}
#    with_items:
#      - git
#      - tree
#      - curl
#      - build-essential
#      - ssh
#      - lzop
#      - rsync
#      - python-dev
#      - python-setuptools
#      - libcurl4-openssl-dev
#      - python-pip
#      - default-jdk
#      - maven
#
#
#  - name: 'Pip installs'
#    pip:
#      name: "{{ item}}"
#    with_items:
#      - virtualenv
#      - virtualenvwrapper
#      - python-dateutil
#
#  - name: Add aliases vagrant .profile
#    blockinfile:
#      dest: /home/vagrant/.profile
#      marker: "# {mark} ANSIBLE MANAGED ALIAS BLOCK "
#      content: |
#        export DOPS=/home/vagrant/shared/devops
#
#  - name: install Hadoop
#    include: includes/hadoop.yml
#
#  - name: install Zookeeper
#    include: includes/zookeeper.yml
#
#  - name: install Accumulo
#    include: includes/accumulo.yml

  - name: install Spark
    include: includes/spark.yml

  - name: get GeoWave
    git:
      repo: https://github.com/ngageoint/geowave.git
      dest: /home/hadoop/geowave
      depth: 10
    become_user: hadoop

  - name: Build GeoWave Accumulo Plugin
    shell: mvn clean install -P accumulo-container-singlejar -DskipITs=true -DskipTests=true -Daccumulo.version=1.7.2 -Dhadoop.version=2.6.0 -Daccumulo.api=1.7
    args:
      chdir: /home/hadoop/geowave
      executable: /bin/bash
    become_user: hadoop

  - name: Copy jar to accumulo
    copy:
      src: /deploy/target/geowave-deploy-0.9.4-SNAPSHOT-accumulo.jar
      dest: /srv/accumulo/lib/
      owner: hadoop
      group: hadoop
      remote_src: yes


# sudo su -l hadoop bash -c 'source /home/hadoop/.bashrc'